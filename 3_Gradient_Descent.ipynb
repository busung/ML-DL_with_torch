{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Gradient Descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXXy+yYEyB79u9bgyDGvyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busung/ML-DL_with_torch/blob/main/3_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P1T_ZqK_wrmH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 루프로 구현해 보는 GD"
      ],
      "metadata": {
        "id": "JSXhJ2BOyNwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])"
      ],
      "metadata": {
        "id": "hdPLPfsIxF0S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.zeros(1)"
      ],
      "metadata": {
        "id": "jY6nbeKpxNqU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1"
      ],
      "metadata": {
        "id": "5UDGTXyaxQAD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cost가 감소하는 것을 확인 할 수 있음\n",
        "#hypothesis와 input,output을 보면 기울기가 1인 직선이기에 W는 1일 경우 Optimal함\n",
        "nb_epochs = 10\n",
        "\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "  hypothesis = W*x_train#우리가 구해야 하는 W\n",
        "\n",
        "  cost = torch.mean((hypothesis - y_train)**2)#W를 추정하기 위해 사용하는 Cost function\n",
        "  gradient = torch.sum((W*x_train-y_train)*x_train)#cost를 미분하여 기울기가 0이 되는 지점을 찾기 위한 Gradient\n",
        "\n",
        "  print(\"epoch:{}, W: {:.3f}, cost: {:.6f}\".format(epoch,W.item(),cost.item()))\n",
        "\n",
        "  W -= lr*gradient#Gradient Descent이므로 기울기가 감소하여 0이되는 지점을 찾아야 하기에 학습율에 gradient를 곱하여 빼줌"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBqAeC-yxQ6l",
        "outputId": "31ca79c0-51b4-45f3-ee80-90d4f40f1e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, W: 0.000, cost: 4.666667\n",
            "epoch:1, W: 1.400, cost: 0.746666\n",
            "epoch:2, W: 0.840, cost: 0.119467\n",
            "epoch:3, W: 1.064, cost: 0.019115\n",
            "epoch:4, W: 0.974, cost: 0.003058\n",
            "epoch:5, W: 1.010, cost: 0.000489\n",
            "epoch:6, W: 0.996, cost: 0.000078\n",
            "epoch:7, W: 1.002, cost: 0.000013\n",
            "epoch:8, W: 0.999, cost: 0.000002\n",
            "epoch:9, W: 1.000, cost: 0.000000\n",
            "epoch:10, W: 1.000, cost: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch의 Optimizer"
      ],
      "metadata": {
        "id": "6fcB7gFSyhK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.zeros(1,requires_grad=True)\n",
        "\n",
        "#Stochastic Gradient Descent를 활용\n",
        "optimizer = optim.SGD([W],lr=0.15)"
      ],
      "metadata": {
        "id": "qnRnm81IyK2I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 10\n",
        "\n",
        "for epoch in range(1,nb_epochs+1):\n",
        "\n",
        "  hypothesis = W*x_train\n",
        "\n",
        "  cost = torch.mean((hypothesis-y_train)**2)\n",
        "\n",
        "  print(\"epoch:{}, W: {:.3f}, cost: {:.6f}\".format(epoch,W.item(),cost.item()))\n",
        "\n",
        "  optimizer.zero_grad()#이 3단계는 늘 함께 해야함\n",
        "  cost.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHjbWwn6zcja",
        "outputId": "8afa9784-ca4d-4757-a787-8390ebdb95c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, W: 0.000, cost: 4.666667\n",
            "epoch:2, W: 1.400, cost: 0.746667\n",
            "epoch:3, W: 0.840, cost: 0.119467\n",
            "epoch:4, W: 1.064, cost: 0.019115\n",
            "epoch:5, W: 0.974, cost: 0.003058\n",
            "epoch:6, W: 1.010, cost: 0.000489\n",
            "epoch:7, W: 0.996, cost: 0.000078\n",
            "epoch:8, W: 1.002, cost: 0.000013\n",
            "epoch:9, W: 0.999, cost: 0.000002\n",
            "epoch:10, W: 1.000, cost: 0.000000\n"
          ]
        }
      ]
    }
  ]
}